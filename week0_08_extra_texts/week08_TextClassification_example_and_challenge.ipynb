{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week08: Text classification with simple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача классификации текстов\n",
    "\n",
    "Задача классификации текстов заключается в том, чтобы определить по документу его класс.\n",
    "В данном случае предлагается рассмотреть в качестве документов - письма, заранее отклассифицированных по 20 темам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = fetch_20newsgroups().target_names\n",
    "all_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём всего 3 темы, но из одного раздела (документы из близких тем сложнее отличать друг от друга)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'sci.electronics',\n",
    "    'sci.space',\n",
    "    'sci.med'\n",
    "]\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train',\n",
    "                                categories=categories,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "test_data = fetch_20newsgroups(subset='test',\n",
    "                               categories=categories,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторизация текстов\n",
    "**Вопрос: как описать текстовые документы пространством признаков?**\n",
    "\n",
    "\n",
    "**Идея №1**: мешок слов (bag-of-words) - каждый документ или текст выглядит как неупорядоченный набор слов без сведений о связях между ними.\n",
    "<img src='https://st2.depositphotos.com/2454953/9959/i/450/depositphotos_99593622-stock-photo-holidays-travel-bag-word-cloud.jpg'>\n",
    "\n",
    "**Идея №2**: создаём вектор \"слов\", каждая компонента отвечает отдельному слову.\n",
    "\n",
    "Для векторизации текстов воспользуемся [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Можно всячески варировать извлечение признаков (убирать редкие слова, убирать частые слова, убирать слова общей лексики, брать биграмы и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, ngram_range=(1, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_matrix = count_vectorizer.fit_transform(train_data.data)\n",
    "sparse_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_2_words = {\n",
    "    v: k\n",
    "    for k, v in count_vectorizer.vocabulary_.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова с наибольшим положительным весом, являются характерными словами темы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся `macro`-average для оценки качества решения в задаче многоклассовой классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scorer = make_scorer(f1_score, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию для предсказания темы документа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = LogisticRegression(C=0.00001)\n",
    "algo.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = algo.coef_.shape[1]\n",
    "for c in algo.classes_:\n",
    "    topic_words = [\n",
    "        num_2_words[w_num]\n",
    "        for w_num in heapq.nlargest(10, range(W), key=lambda w: algo.coef_[c, w])\n",
    "    ]\n",
    "    print(',  '.join(topic_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним качество на обучающей и отложенной выборках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f_scorer(algo, sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scorer(algo, count_vectorizer.transform(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Значения f-меры получились очень низкие.\n",
    "\n",
    "**Вопрос:** в чём причина?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(algo.coef_[0], bins=500)\n",
    "plt.xlim([-0.0006, 0.0006])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Какую выбрать метрику для регуляризации? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = LogisticRegression(penalty='l1', C=0.1)\n",
    "arr = cross_val_score(algo, sparse_feature_matrix, train_data.target, cv=5, scoring=f_scorer)\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f_scorer(algo, sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scorer(algo, count_vectorizer.transform(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подберём оптимальное значение параметра регуляризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_plot(x, y, x_label, title, y_label='f_measure'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.grid(True),\n",
    "    plt.plot(x, y, 'go-')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*map(float, np.logspace(-2, 2, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid = {\n",
    "    'C': np.logspace(-2, 2, 10),\n",
    "}\n",
    "gs = GridSearchCV(LogisticRegression(penalty='l1'), lr_grid, scoring=f_scorer, cv=5, n_jobs=5)\n",
    "%time  gs.fit(sparse_feature_matrix, train_data.target)\n",
    "print(\"best_params: {}, best_score: {}\".format(gs.best_params_, gs.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим график:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_plot(\n",
    "    lr_grid['C'], gs.cv_results_['mean_test_score'], 'C - coefficient of regularization', 'LogReg(penalty=l1)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_grid = {\n",
    "    'C': np.linspace(1, 20, 40),\n",
    "}\n",
    "gs = GridSearchCV(LogisticRegression(penalty='l1'), lr_grid, scoring=f_scorer, cv=5, n_jobs=5)\n",
    "%time  gs.fit(sparse_feature_matrix, train_data.target)\n",
    "print(\"best_params: {}, best_score: {}\".format(gs.best_params_, gs.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_plot(\n",
    "    lr_grid['C'], gs.cv_results_['mean_test_score'], 'C - coefficient of regularization', 'LogReg(penalty=l1)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final = LogisticRegression(penalty='l1', C=10)\n",
    "%time lr_final.fit(sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(lr_final.predict(sparse_feature_matrix), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f_scorer(lr_final, sparse_feature_matrix, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(lr_final.predict(count_vectorizer.transform(test_data.data)), test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scorer(lr_final, count_vectorizer.transform(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регуляризация вместе с векторизацией признаков\n",
    "Чтобы не делать векторизацию и обучение раздельно, есть удобный класс Pipeline. Он позволяет объединить в цепочку последовательность действий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer(min_df=5, ngram_range=(1, 2))),\n",
    "    (\"algo\", LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f_scorer(pipeline, train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scorer(pipeline, test_data.data, test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значения такие же как мы получали ранее, делая шаги раздельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При кроссвалидации нужно, чтобы CountVectorizer не обучался на тесте (иначе объекты становятся зависимыми). Pipeline позволяет это просто сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=5, ngram_range=(1, 2)), LogisticRegression())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=5, scoring=f_scorer)\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В Pipeline можно добавлять новые шаги препроцессинга данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(CountVectorizer(min_df=5, ngram_range=(1, 2)), TfidfTransformer(), LogisticRegression())\n",
    "arr = cross_val_score(pipeline, train_data.data, train_data.target, cv=5, scoring=f_scorer)\n",
    "print(arr)\n",
    "print(np.mean(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(pipeline.predict(train_data.data), train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f_scorer(pipeline, train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(pipeline.predict(test_data.data), test_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scorer(pipeline, test_data.data, test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество стало немного лучше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация сообщений чатов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве задания предлагается построить модель классификации текстов, соответствующих сообщениям из чатов по ML, Python и знакомствам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Данные** можно взять с <a src=\"https://www.kaggle.com/c/tfstextclassification\">соревнования на Kaggle</a>, проведенное в рамках курса \"Диалоговые системы\" в Тинькофф. Прямая [ссылка](https://www.dropbox.com/s/8wckwzfy63ajxpm/tfstextclassification.zip?dl=0) на скачивание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'data/{}'\n",
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Первичный анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 0\n",
    "print('Label: ', label, '\\n'+'='*100+'\\n')\n",
    "print(*df[df['label'] == label].sample(10).text, sep='\\n'+'-'*100+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 1\n",
    "print('Label: ', label, '\\n'+'='*100+'\\n')\n",
    "print(*df[df['label'] == label].sample(10).text, sep='\\n'+'-'*100+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 2\n",
    "print('Label: ', label, '\\n'+'='*100+'\\n')\n",
    "print(*df[df['label'] == label].sample(10).text, sep='\\n'+'-'*100+'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделим данные на train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(3, random_state=37)\n",
    "train_index, test_index = next(skf.split(df.text, df.label))\n",
    "train_df, test_df = df.iloc[train_index], df.iloc[test_index]\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.text\n",
    "y_train = train_df.label\n",
    "print(X_train.shape)\n",
    "\n",
    "X_test = test_df.text\n",
    "y_test = test_df.label\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", CountVectorizer()),\n",
    "    (\"clf\", DecisionTreeClassifier()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим классификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "clf = pipeline\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train_acc: {:.4f}, train_f-measure: {:.4f}\".format(\n",
    "    accuracy_score(clf.predict(X_train), y_train),\n",
    "    f_scorer(clf, X_train, y_train)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test_acc: {:.4f}, test_f-measure: {:.4f}\".format(\n",
    "    accuracy_score(clf.predict(X_test), y_test),\n",
    "    f_scorer(clf, X_test, y_test)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "Как видим, наша модель переобучилась. Для получения лучших результатов попробуйте воспользоваться более хитрыми и походящими инструментами.\n",
    "\n",
    "1. Попробуйте поработать с параметрами `CountVectorizer`.\n",
    "2. Попробуйте воспользоваться TF-IDF для кодирования текстовой информации ([ссылка](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)).\n",
    "3. Попробуйте воспользоваться другими моделями и средствами снижения размерности.\n",
    "\n",
    "Формальный критерий успешности выполнения данного (опционального) задания: \n",
    "* Проведен честный эксперимент с апробацией различных методов (>=3)\n",
    "* Полученный алгоритм не выказывает явных следов переобучения (качество на train и test не различаются более, чем на 0.03 условных попугая)\n",
    "* Test accuracy >= 0.835, f1-score >= 0.815"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
