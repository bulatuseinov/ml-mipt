[recap] Attention basics:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ml-mipt/ml-mipt/blob/advanced/week03_NMT_and_attention/week03_extra_Attention_basics.ipynb)

Full Transformer architecture and training pipeline by Harvard NLP:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb)

Understanding the positional encoding:
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ml-mipt/ml-mipt/blob/advanced/week04_Transformer/week04_positional_encoding_carriers.ipynb)



__Further readings__:
* [en] The Illustrated Transformer [blog post](https://jalammar.github.io/illustrated-transformer/)

* [en] Harvard NLP [full implementation in PyTorch](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

* [en] OpenAI blog post [Better Language Models
and Their Implications (GPT-2)](https://openai.com/blog/better-language-models/)

* [en] Paper describing positional encoding ["Convolutional Sequence to Sequence Learning"](https://arxiv.org/pdf/1705.03122)

* [en] Paper presenting [Layer Normalization](https://arxiv.org/abs/1607.06450)