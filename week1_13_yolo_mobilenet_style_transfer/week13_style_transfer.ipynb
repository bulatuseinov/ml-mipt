{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAv_IoHU6poF"
   },
   "source": [
    "### Neural style transfer in PyTorch\n",
    "\n",
    "This tutorial implements the \"slow\" neural style transfer based on the AlexNet model.\n",
    "However, you can also use any model from VGG family from torchvision.models.\n",
    "\n",
    "Please note, that any other model from torchvision.models will not work properly with this code due to the complex model class internal structure.\n",
    "\n",
    "_Reference:based on [YSDA Materials](https://github.com/yandexdataschool/Practical_DL/blob/spring2019/week06_style_transfer/style_transfer_pytorch.ipynb). Closely follows the official neural style tutorial you can find [here](http://pytorch.org/tutorials/advanced/neural_style_tutorial.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6K3pbm1V6poO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.ndimage as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hbx6vD1u6poQ"
   },
   "outputs": [],
   "source": [
    "# desired size of the output image\n",
    "imsize = 512   # REDUCE THIS TO 128 IF THE OPTIMIZATION IS TOO SLOW FOR YOU\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name).resize((imsize, imsize), Image.ANTIALIAS)\n",
    "    image = np.transpose(np.array(image), (2,0,1))\n",
    "    image = image / image.max()\n",
    "    image = Variable(dtype(image))\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"torch\", torch.__version__)\n",
    "if use_cuda:\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"Not using GPU.\")\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('images', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVcNnRjt6poU"
   },
   "outputs": [],
   "source": [
    "!wget -q https://i.kym-cdn.com/entries/icons/mobile/000/030/157/womanyellingcat.jpg -O images/my_img.jpg\n",
    "!wget -q https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/450px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg -O images/art.jpg\n",
    "content_img = image_loader(\"images/my_img.jpg\").type(dtype)\n",
    "style_img = image_loader(\"images/art.jpg\").type(dtype)\n",
    "\n",
    "assert style_img.size() == content_img.size(), \\\n",
    "    \"we need to import style and content images of the same size\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezvDYJT46pof"
   },
   "source": [
    "### Draw input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ppWvm0dJ6pof"
   },
   "outputs": [],
   "source": [
    "def imshow(tensor, title=None):\n",
    "    image = tensor.detach().clone().cpu()  # we clone the tensor to not do changes on it\n",
    "    image = image.view(3, imsize, imsize)  # remove the fake batch dimension\n",
    "    image = image.numpy().transpose([1,2,0])\n",
    "    plt.imshow(image / np.max(image))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "plt.figure(figsize=[12,6])\n",
    "plt.subplot(1,2,1)\n",
    "imshow(style_img.data, title='Style Image')\n",
    "plt.subplot(1,2,2)\n",
    "imshow(content_img.data, title='Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g3YBH3CL6poi"
   },
   "source": [
    "### Define Style Transfer Losses\n",
    "\n",
    "Let's define two loss functions: content and style losses.\n",
    "\n",
    "Content loss is simply a pointwise mean squared error of high-level features while style loss is the error between gram matrices of intermediate feature layers.\n",
    "\n",
    "To obtain the feature representations we use a pre-trained VGG19 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6oPr0mgN6poi"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "cnn = models.alexnet(pretrained=True).features\n",
    "\n",
    "# move it to the GPU if possible:\n",
    "if use_cuda:\n",
    "    cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3cR4Fsy6pok"
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, weight):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input * self.weight, self.target)\n",
    "        return input.clone()\n",
    "\n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-EGKvAxw6pom"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(in_tensor):\n",
    "    B, C, H, W = in_tensor.size() \n",
    "    # B=batch size(=1)\n",
    "    # C=number of feature maps\n",
    "    # (H,W)=dimensions of a feature map (N=c*d)\n",
    "\n",
    "    features = in_tensor.view(B * C, H * W)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(B * C * H * W)\n",
    "    \n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.G = gram_matrix(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = F.mse_loss(self.G, self.target)\n",
    "        return input.clone()\n",
    "\n",
    "    def backward(self, retain_graph=True):\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aO-VIv4D6poo"
   },
   "source": [
    "### Style transfer pipeline\n",
    "\n",
    "We can now define a unified \"model\" that computes all the losses on the image triplet (content image, style image, optimized image) so that we could optimize them with backprop (over image pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FavddWZT6poo"
   },
   "outputs": [],
   "source": [
    "content_weight=1            # coefficient for content loss\n",
    "style_weight=1000           # coefficient for style loss\n",
    "content_layers=('conv_4',)  # use these layers for content loss\n",
    "style_layers=('conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5') # use these layers for style loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yXMjxMm6poq"
   },
   "outputs": [],
   "source": [
    "content_losses = []\n",
    "style_losses = []\n",
    "\n",
    "model = nn.Sequential()  # the new Sequential module network\n",
    "# move these modules to the GPU if possible:\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "i = 1\n",
    "for layer in list(cnn):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        name = \"conv_\" + str(i)\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).clone()\n",
    "            content_loss = ContentLoss(target, content_weight)\n",
    "            model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).clone()\n",
    "            target_feature_gram = gram_matrix(target_feature)\n",
    "            style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "            model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    if isinstance(layer, nn.ReLU):\n",
    "        name = \"relu_\" + str(i)\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).clone()\n",
    "            content_loss = ContentLoss(target, content_weight)\n",
    "            model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).clone()\n",
    "            target_feature_gram = gram_matrix(target_feature)\n",
    "            style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "            model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if isinstance(layer, nn.MaxPool2d):\n",
    "        name = \"pool_\" + str(i)\n",
    "        model.add_module(name, layer)  # ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YChemFGO6pos"
   },
   "source": [
    "### Optimization\n",
    "\n",
    "We can now optimize both style and content loss over input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BDixv8j_6pos"
   },
   "outputs": [],
   "source": [
    "input_image = Variable(content_img.clone().data, requires_grad=True)\n",
    "optimizer = torch.optim.LBFGS([input_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1okfmFEq6pou",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 300\n",
    "\n",
    "for i in range(num_steps):\n",
    "    # correct the values of updated input image\n",
    "    input_image.data.clamp_(0, 1)\n",
    "\n",
    "    res = model(input_image)\n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "    for sl in style_losses:\n",
    "        style_score += sl.backward()\n",
    "    for cl in content_losses:\n",
    "        content_score += cl.backward()\n",
    "        \n",
    "    if i % 10 == 0:  # <--- adjust the value to see updates more frequently\n",
    "        \n",
    "        print('Step # {} Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "            i, style_score.detach().cpu().numpy(), content_score.detach().cpu().numpy()))\n",
    "        plt.figure(figsize=[10,10])\n",
    "        imshow(input_image)\n",
    "        plt.show()\n",
    "        \n",
    "    loss = style_score + content_score\n",
    "    \n",
    "    optimizer.step(lambda:loss)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "# a last correction...\n",
    "input_image.data = input_image.data.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-L7wyG6g6pow"
   },
   "source": [
    "### Final image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7h6z2aS6poy"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10,10])\n",
    "imshow(input_image.data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YTxJzBo6poW"
   },
   "source": [
    "# Let's Dream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypJvy0JhFPOy"
   },
   "source": [
    "That's what happens when you play with some ideas, that are proposed [here](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpkIJryf6poW"
   },
   "outputs": [],
   "source": [
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "preprocess = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "def deprocess(image_np):\n",
    "    image_np = image_np.squeeze().transpose(1, 2, 0)\n",
    "    image_np = image_np * std.reshape((1, 1, 3)) + mean.reshape((1, 1, 3))\n",
    "    image_np = np.clip(image_np, 0.0, 255.0)\n",
    "    return image_np\n",
    "\n",
    "def clip(image_tensor):\n",
    "    for c in range(3):\n",
    "        m, s = mean[c], std[c]\n",
    "        image_tensor[0, c] = torch.clamp(image_tensor[0, c], -m / s, (1 - m) / s)\n",
    "    return image_tensor\n",
    "\n",
    "def dream(image, model, iterations, lr):\n",
    "    \"\"\" Updates the image to maximize outputs for n iterations \"\"\"\n",
    "    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available else torch.FloatTensor\n",
    "    image = Variable(Tensor(image), requires_grad=True)\n",
    "    for i in range(iterations):\n",
    "        model.zero_grad()\n",
    "        out = model(image)\n",
    "        loss = out.norm()\n",
    "        loss.backward()\n",
    "        avg_grad = np.abs(image.grad.data.cpu().numpy()).mean()\n",
    "        norm_lr = lr / avg_grad\n",
    "        image.data += norm_lr * image.grad.data\n",
    "        image.data = clip(image.data)\n",
    "        image.grad.data.zero_()\n",
    "    return image.cpu().data.numpy()\n",
    "\n",
    "\n",
    "def deep_dream(image, model, iterations, lr, octave_scale, num_octaves):\n",
    "    \"\"\" Main Deep Dream processing \"\"\"\n",
    "    image = preprocess(image).unsqueeze(0).cpu().data.numpy()\n",
    "\n",
    "    # Extract image representations for each octave\n",
    "    octaves = [image]\n",
    "    for _ in range(num_octaves - 1):\n",
    "        octaves.append(nd.zoom(octaves[-1], (1, 1, 1 / octave_scale, 1 / octave_scale), order=1))\n",
    "\n",
    "    detail = np.zeros_like(octaves[-1])\n",
    "    for octave, octave_base in enumerate(tqdm(octaves[::-1], desc=\"Dreaming\")):\n",
    "        if octave > 0:\n",
    "            # Upsample detail to new octave dimension\n",
    "            detail = nd.zoom(detail, np.array(octave_base.shape) / np.array(detail.shape), order=1)\n",
    "        # Add deep dream detail from previous octave to new base\n",
    "        input_image = octave_base + detail\n",
    "        # Get new deep dream image\n",
    "        dreamed_image = dream(input_image, model, iterations, lr)\n",
    "        # Extract deep dream details\n",
    "        detail = dreamed_image - octave_base\n",
    "\n",
    "    return deprocess(dreamed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1Vg4K12BUWd"
   },
   "outputs": [],
   "source": [
    "# you can play with the parameters\n",
    "layer_num = 32\n",
    "iterations = 100\n",
    "octave_scale = 1.2\n",
    "num_octaves = 3\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j62lbGEtBgA3"
   },
   "outputs": [],
   "source": [
    "image = Image.open(\"images/my_img.jpg\").resize((256, 256))\n",
    "# Define the model\n",
    "network = models.vgg19(pretrained=True)\n",
    "layers = list(network.features.children())\n",
    "model = nn.Sequential(*layers[: (layer_num + 1)])\n",
    "if torch.cuda.is_available:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-r9XVk-BjoA"
   },
   "outputs": [],
   "source": [
    "dreamed_image = deep_dream(\n",
    "    image,\n",
    "    model,\n",
    "    iterations=iterations,\n",
    "    lr=lr,\n",
    "    octave_scale=octave_scale,\n",
    "    num_octaves=num_octaves\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UCva8SPBjrU"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(dreamed_image)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "style_transfer_pytorch-draft.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
